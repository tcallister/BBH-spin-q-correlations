{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob, json\n",
    "import sys\n",
    "sys.path.append('./../code/')\n",
    "from support import *\n",
    "from likelihoods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Load successfully found injections, which we will reweight to our proposed population fits in preparation for predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:5: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:6: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:7: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:9: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:10: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:11: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:12: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "/cvmfs/oasis.opensciencegrid.org/ligo/sw/conda/envs/igwn-py37/lib/python3.7/site-packages/ipykernel/__main__.py:13: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n"
     ]
    }
   ],
   "source": [
    "# Load mock detections\n",
    "mMin = 5.\n",
    "\n",
    "mockDetections = h5py.File('../input/o3a_bbhpop_inj_info.hdf','r')\n",
    "ifar_1 = mockDetections['injections']['ifar_gstlal'].value\n",
    "ifar_2 = mockDetections['injections']['ifar_pycbc_bbh'].value\n",
    "ifar_3 = mockDetections['injections']['ifar_pycbc_full'].value\n",
    "detected = (ifar_1>1) + (ifar_2>1) + (ifar_3>1)\n",
    "m1_det = mockDetections['injections']['mass1_source'].value[detected]\n",
    "m2_det = mockDetections['injections']['mass2_source'].value[detected]\n",
    "s1z_det = mockDetections['injections']['spin1z'].value[detected]\n",
    "s2z_det = mockDetections['injections']['spin2z'].value[detected]\n",
    "z_det = mockDetections['injections']['redshift'].value[detected]\n",
    "\n",
    "mockDetectionsO1O2 = h5py.File('../input/injections_O1O2an_spin.h5','r')\n",
    "m1_det = np.append(m1_det,mockDetectionsO1O2['mass1_source'])\n",
    "m2_det = np.append(m2_det,mockDetectionsO1O2['mass2_source'])\n",
    "s1z_det = np.append(s1z_det,mockDetectionsO1O2['spin1z'])\n",
    "s2z_det = np.append(s2z_det,mockDetectionsO1O2['spin2z'])\n",
    "z_det = np.append(z_det,mockDetectionsO1O2['redshift'])\n",
    "\n",
    "# Derived quantities\n",
    "q_det = m2_det/m1_det\n",
    "X_det = (m1_det*s1z_det + m2_det*s2z_det)/(m1_det+m2_det)\n",
    "\n",
    "pop_reweight = injection_weights(m1_det,m2_det,s1z_det,s2z_det,z_det,mMin=5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 1. Population with no mass ratio-spin correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_catalogs = 750\n",
    "posteriors = np.load(\"../input/sampleDict.pickle\",allow_pickle=True)\n",
    "samps_no_evol = np.load('../code/output/processed_emcee_samples_plPeak_noEvol_r00.npy')\n",
    "\n",
    "# Prepare arrays to hold mock draws from injection sets and reweighted posteriors\n",
    "mock_q_noEvol = np.zeros((len(posteriors),n_catalogs))\n",
    "mock_x_noEvol = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_q_noEvol = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_x_noEvol = np.zeros((len(posteriors),n_catalogs))\n",
    "\n",
    "mu_chi = np.zeros(n_catalogs)\n",
    "logsig_chi = np.zeros(n_catalogs)\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    samp = np.random.choice(np.arange(samps_no_evol.shape[0]))\n",
    "    \n",
    "    mMin = 5.\n",
    "    lmbda = samps_no_evol[samp,0]\n",
    "    mMax = samps_no_evol[samp,1]\n",
    "    m0 = samps_no_evol[samp,2]\n",
    "    sigM = samps_no_evol[samp,3]\n",
    "    fPeak = samps_no_evol[samp,4]\n",
    "    bq = samps_no_evol[samp,5]\n",
    "    kappa = samps_no_evol[samp,6]\n",
    "    mu0 = samps_no_evol[samp,7]\n",
    "    logsig0 = samps_no_evol[samp,8]\n",
    "    \n",
    "    mu_chi[i] = mu0\n",
    "    logsig_chi[i] = logsig0\n",
    "    \n",
    "    # New p(m1)\n",
    "    p_det_m1_pl = (1.+lmbda)*m1_det**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "    p_det_m1_pl[m1_det>mMax] = 0\n",
    "    p_det_m1_peak = np.exp(-0.5*(m1_det-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "    mock_p_m1 = fPeak*p_det_m1_peak + (1.-fPeak)*p_det_m1_pl\n",
    "    \n",
    "    # New p(m2)\n",
    "    mock_p_m2 = (1.+bq)*np.power(m2_det,bq)/(np.power(m1_det,1.+bq)-mMin**(1.+bq))\n",
    "    mock_p_m2[m2_det<mMin]=0\n",
    "    \n",
    "    # New p(z) and p(chi)\n",
    "    mock_p_z = np.power(1.+z_det,kappa-1.)\n",
    "    mock_p_chi = calculate_Gaussian(X_det, mu0, 10.**(2.*logsig0),-1.,1.)\n",
    "    \n",
    "    p_det = mock_p_m1*mock_p_m2*mock_p_z*mock_p_chi*pop_reweight\n",
    "    p_det /= np.sum(p_det)\n",
    "    \n",
    "    # Given detection weights, draw a mock event\n",
    "    try:\n",
    "        detected_injections = np.random.choice(np.arange(m1_det.size),size=len(posteriors),p=p_det,replace=True)\n",
    "        \n",
    "    except ValueError:\n",
    "        print(samp)\n",
    "        \n",
    "    # Save selected value\n",
    "    mock_q_noEvol[:,i] = q_det[detected_injections]\n",
    "    mock_x_noEvol[:,i] = X_det[detected_injections]\n",
    "\n",
    "    # Now loop across events\n",
    "    for ii,key in enumerate(posteriors):\n",
    "\n",
    "        # Read out properties\n",
    "        chis = posteriors[key]['Xeff']\n",
    "        Xeff_prior = posteriors[key]['Xeff_priors']\n",
    "        m1s = posteriors[key]['m1']\n",
    "        m2s = posteriors[key]['m2']\n",
    "        zs = posteriors[key]['z']\n",
    "        weights = posteriors[key]['weights']\n",
    "        qs = m2s/m1s\n",
    "\n",
    "        p_Chi = calculate_Gaussian(chis, mu0, 10.**(2.*logsig0),-1.,1.)\n",
    "\n",
    "        # p(m1)\n",
    "        p_m1_pl = (1.+lmbda)*m1s**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "        p_m1_pl[m1s>mMax] = 0.\n",
    "        p_m1_peak = np.exp(-0.5*(m1s-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "        p_m1 = fPeak*p_m1_peak + (1.-fPeak)*p_m1_pl\n",
    "        old_m1_prior = np.ones(m1s.size)\n",
    "\n",
    "        # p(m2)\n",
    "        p_m2 = (1.+bq)*np.power(m2s,bq)/(np.power(m1s,1.+bq)-mMin**(1.+bq))\n",
    "        old_m2_prior = np.ones(m1s.size)\n",
    "        p_m2[m2s<mMin]=0\n",
    "\n",
    "        # p(z)\n",
    "        p_z = np.power(1.+zs,kappa-1.)\n",
    "        old_pz_prior = (1.+zs)**(2.7-1.)\n",
    "        \n",
    "        # Draw probs\n",
    "        probs = p_Chi*p_m1*p_m2*p_z*weights/Xeff_prior/old_m1_prior/old_m2_prior/old_pz_prior\n",
    "        \n",
    "        if np.any(probs!=probs):\n",
    "            print(np.where(probs!=probs))\n",
    "            print(probs)\n",
    "        probs[probs<0] = 0.\n",
    "        probs /= np.sum(probs)\n",
    "        chosenInd = np.random.choice(np.arange(m1s.size),p=probs)\n",
    "        \n",
    "        # Save selected sample\n",
    "        resampled_q_noEvol[ii,i] = qs[chosenInd]\n",
    "        resampled_x_noEvol[ii,i] = chis[chosenInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reweighted_sample_dict = {\n",
    "    'mock_q':mock_q_noEvol,\n",
    "    'mock_chi':mock_x_noEvol,\n",
    "    'resampled_q':resampled_q_noEvol,\n",
    "    'resampled_chi':resampled_x_noEvol\n",
    "}\n",
    "\n",
    "np.save('./reweighted_samples_noEvolution.npy',reweighted_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 2. Population *with* chi-q correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_catalogs = 750\n",
    "posteriors = np.load(\"../input/sampleDict.pickle\",allow_pickle=True)\n",
    "samps = np.load('../code/output/processed_emcee_samples_plPeak_r00.npy')\n",
    "\n",
    "mock_q = np.zeros((len(posteriors),n_catalogs))\n",
    "mock_x = np.zeros((len(posteriors),n_catalogs))\n",
    "\n",
    "resampled_q = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_x = np.zeros((len(posteriors),n_catalogs))\n",
    "\n",
    "mu_chi_evol = np.zeros(n_catalogs)\n",
    "logsig_chi_evol = np.zeros(n_catalogs)\n",
    "alpha_evol = np.zeros(n_catalogs)\n",
    "beta_evol = np.zeros(n_catalogs)\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    samp = np.random.choice(np.arange(samps.shape[0]))\n",
    "    \n",
    "    mMin = 5.\n",
    "    lmbda = samps[samp,0]\n",
    "    mMax = samps[samp,1]\n",
    "    m0 = samps[samp,2]\n",
    "    sigM = samps[samp,3]\n",
    "    fPeak = samps[samp,4]\n",
    "    bq = samps[samp,5]\n",
    "    kappa = samps[samp,6]\n",
    "    mu0 = samps[samp,7]\n",
    "    logsig0 = samps[samp,8]\n",
    "    alpha = samps[samp,9]\n",
    "    beta = samps[samp,10]\n",
    "    \n",
    "    mu_chi_evol[i] = mu0\n",
    "    logsig_chi_evol[i] = logsig0    \n",
    "    alpha_evol[i] = alpha\n",
    "    beta_evol[i] = beta\n",
    "    \n",
    "    p_det_m1_pl = (1.+lmbda)*m1_det**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "    p_det_m1_pl[m1_det>mMax] = 0\n",
    "    p_det_m1_peak = np.exp(-0.5*(m1_det-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "    mock_p_m1 = fPeak*p_det_m1_peak + (1.-fPeak)*p_det_m1_pl \n",
    "    \n",
    "    mock_p_m2 = (1.+bq)*np.power(m2_det,bq)/(np.power(m1_det,1.+bq)-mMin**(1.+bq))\n",
    "    mock_p_m2[m2_det<mMin]=0\n",
    "    \n",
    "    mock_p_z = np.power(1.+z_det,kappa-1.)\n",
    "    \n",
    "    mu = mu0 + alpha*(q_det-0.5)\n",
    "    logsig = logsig0 + beta*(q_det-0.5)\n",
    "    mock_p_chi = calculate_Gaussian(X_det, mu, 10.**(2.*logsig),-1.,1.)\n",
    "    \n",
    "    p_det = mock_p_m1*mock_p_m2*mock_p_z*mock_p_chi*pop_reweight\n",
    "    p_det /= np.sum(p_det)\n",
    "    detected_injections = np.random.choice(np.arange(m1_det.size),size=len(posteriors),p=p_det,replace=False)\n",
    "    mock_q[:,i] = q_det[detected_injections]\n",
    "    mock_x[:,i] = X_det[detected_injections]\n",
    "\n",
    "    for ii,key in enumerate(posteriors):\n",
    "\n",
    "        chis = posteriors[key]['Xeff']\n",
    "        Xeff_prior = posteriors[key]['Xeff_priors']\n",
    "        m1s = posteriors[key]['m1']\n",
    "        m2s = posteriors[key]['m2']\n",
    "        zs = posteriors[key]['z']\n",
    "        weights = posteriors[key]['weights']\n",
    "        qs = m2s/m1s\n",
    "        \n",
    "        mu = mu0 + alpha*(qs-0.5)\n",
    "        logsig = logsig0 + beta*(qs-0.5)\n",
    "        p_Chi = calculate_Gaussian(chis, mu, 10.**(2.*logsig),-1.,1.)\n",
    "\n",
    "        # p(m1)\n",
    "        p_m1_pl = (1.+lmbda)*m1s**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "        p_m1_pl[m1s>mMax] = 0.\n",
    "        p_m1_peak = np.exp(-0.5*(m1s-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "        p_m1 = fPeak*p_m1_peak + (1.-fPeak)*p_m1_pl\n",
    "        old_m1_prior = np.ones(m1s.size)\n",
    "\n",
    "        # p(m2)\n",
    "        p_m2 = (1.+bq)*np.power(m2s,bq)/(np.power(m1s,1.+bq)-mMin**(1.+bq))\n",
    "        old_m2_prior = np.ones(m1s.size)\n",
    "        p_m2[m2s<mMin]=0\n",
    "\n",
    "        # p(z)\n",
    "        p_z = np.power(1.+zs,kappa-1.)\n",
    "        old_pz_prior = (1.+zs)**(2.7-1.)\n",
    "        \n",
    "        # Draw probs\n",
    "        probs = p_Chi*p_m1*p_m2*p_z*weights/Xeff_prior/old_m1_prior/old_m2_prior/old_pz_prior\n",
    "        \n",
    "        if np.any(probs!=probs):\n",
    "            print(np.where(probs!=probs))\n",
    "            print(probs)\n",
    "        probs[probs<0] = 0.\n",
    "        probs /= np.sum(probs)\n",
    "        chosenInd = np.random.choice(np.arange(m1s.size),p=probs)\n",
    "        \n",
    "        resampled_q[ii,i] = qs[chosenInd]\n",
    "        resampled_x[ii,i] = chis[chosenInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reweighted_sample_dict = {\n",
    "    'mock_q':mock_q,\n",
    "    'mock_chi':mock_x,\n",
    "    'resampled_q':resampled_q,\n",
    "    'resampled_chi':resampled_x\n",
    "}\n",
    "\n",
    "np.save('./reweighted_samples_yesEvolution.npy',reweighted_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 3. Population with spin-q correlations, neglecting GW190412"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_catalogs = 750\n",
    "posteriors = np.load(\"../input/sampleDict.pickle\",allow_pickle=True)\n",
    "posteriors.pop('S190412m')\n",
    "samps_no190412 = np.load('../code/output/processed_emcee_samples_plPeak_no190412_r00.npy')\n",
    "\n",
    "mock_q_no190412 = np.zeros((len(posteriors),n_catalogs))\n",
    "mock_x_no190412 = np.zeros((len(posteriors),n_catalogs))\n",
    "\n",
    "resampled_q_no190412 = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_x_no190412 = np.zeros((len(posteriors),n_catalogs))\n",
    "\n",
    "mu_chi_evol_no190412 = np.zeros(n_catalogs)\n",
    "logsig_chi_evol_no190412 = np.zeros(n_catalogs)\n",
    "alpha_evol_no190412 = np.zeros(n_catalogs)\n",
    "beta_evol_no190412 = np.zeros(n_catalogs)\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    samp = np.random.choice(np.arange(samps_no190412.shape[0]))\n",
    "\n",
    "    mMin = 5.\n",
    "    lmbda = samps_no190412[samp,0]\n",
    "    mMax = samps_no190412[samp,1]\n",
    "    m0 = samps_no190412[samp,2]\n",
    "    sigM = samps_no190412[samp,3]\n",
    "    fPeak = samps_no190412[samp,4]\n",
    "    bq = samps_no190412[samp,5]\n",
    "    kappa = samps_no190412[samp,6]\n",
    "    mu0 = samps_no190412[samp,7]\n",
    "    logsig0 = samps_no190412[samp,8]\n",
    "    alpha = samps_no190412[samp,9]\n",
    "    beta = samps_no190412[samp,10]\n",
    "\n",
    "    mu_chi_evol_no190412[i] = mu0\n",
    "    logsig_chi_evol_no190412[i] = logsig0    \n",
    "    alpha_evol_no190412[i] = alpha\n",
    "    beta_evol_no190412[i] = beta\n",
    "\n",
    "    p_det_m1_pl = (1.+lmbda)*m1_det**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "    p_det_m1_pl[m1_det>mMax] = 0\n",
    "    p_det_m1_peak = np.exp(-0.5*(m1_det-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "    mock_p_m1 = fPeak*p_det_m1_peak + (1.-fPeak)*p_det_m1_pl  \n",
    "\n",
    "    mock_p_m2 = (1.+bq)*np.power(m2_det,bq)/(np.power(m1_det,1.+bq)-mMin**(1.+bq))\n",
    "    mock_p_m2[m2_det<mMin]=0\n",
    "\n",
    "    mock_p_z = np.power(1.+z_det,kappa-1.)\n",
    "\n",
    "    mu = mu0 + alpha*(q_det-0.5)\n",
    "    logsig = logsig0 + beta*(q_det-0.5)\n",
    "    mock_p_chi = calculate_Gaussian(X_det, mu, 10.**(2.*logsig),-1.,1.)\n",
    "\n",
    "    p_det = mock_p_m1*mock_p_m2*mock_p_z*mock_p_chi*pop_reweight\n",
    "    p_det /= np.sum(p_det)\n",
    "\n",
    "    detected_injections = np.random.choice(np.arange(m1_det.size),size=len(posteriors),p=p_det,replace=False)\n",
    "    mock_q_no190412[:,i] = q_det[detected_injections]\n",
    "    mock_x_no190412[:,i] = X_det[detected_injections]\n",
    "\n",
    "    for ii,key in enumerate(posteriors):\n",
    "\n",
    "        chis = posteriors[key]['Xeff']\n",
    "        Xeff_prior = posteriors[key]['Xeff_priors']\n",
    "        m1s = posteriors[key]['m1']\n",
    "        m2s = posteriors[key]['m2']\n",
    "        zs = posteriors[key]['z']\n",
    "        weights = posteriors[key]['weights']\n",
    "        qs = m2s/m1s\n",
    "\n",
    "        mu = mu0 + alpha*(qs-0.5)\n",
    "        logsig = logsig0 + beta*(qs-0.5)\n",
    "        p_Chi = calculate_Gaussian(chis, mu, 10.**(2.*logsig),-1.,1.)\n",
    "\n",
    "        # p(m1)\n",
    "        p_m1_pl = (1.+lmbda)*m1s**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "        p_m1_pl[m1s>mMax] = 0.\n",
    "        p_m1_peak = np.exp(-0.5*(m1s-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "        p_m1 = fPeak*p_m1_peak + (1.-fPeak)*p_m1_pl\n",
    "        old_m1_prior = np.ones(m1s.size)\n",
    "\n",
    "        # p(m2)\n",
    "        p_m2 = (1.+bq)*np.power(m2s,bq)/(np.power(m1s,1.+bq)-mMin**(1.+bq))\n",
    "        old_m2_prior = np.ones(m1s.size)\n",
    "        p_m2[m2s<mMin]=0\n",
    "\n",
    "        # p(z)\n",
    "        p_z = np.power(1.+zs,kappa-1.)\n",
    "        old_pz_prior = (1.+zs)**(2.7-1.)\n",
    "\n",
    "        # Draw probs\n",
    "        probs = p_Chi*p_m1*p_m2*p_z*weights/Xeff_prior/old_m1_prior/old_m2_prior/old_pz_prior\n",
    "\n",
    "        if np.any(probs!=probs):\n",
    "            print(np.where(probs!=probs))\n",
    "            print(probs)\n",
    "        probs[probs<0] = 0.\n",
    "        probs /= np.sum(probs)\n",
    "\n",
    "        chosenInd = np.random.choice(np.arange(m1s.size),p=probs)\n",
    "        resampled_q_no190412[ii,i] = qs[chosenInd]\n",
    "        resampled_x_no190412[ii,i] = chis[chosenInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reweighted_sample_dict = {\n",
    "    'mock_q':mock_q_no190412,\n",
    "    'mock_chi':mock_x_no190412,\n",
    "    'resampled_q':resampled_q_no190412,\n",
    "    'resampled_chi':resampled_x_no190412\n",
    "}\n",
    "\n",
    "np.save('./reweighted_samples_yesEvolution_no190412.npy',reweighted_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 4. Correlated population including GW190814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_catalogs = 750\n",
    "posteriors_w190814 = np.load(\"../input/sampleDict_w190814.pickle\",allow_pickle=True)\n",
    "samps_w190814 = np.load('../code/output/processed_emcee_samples_plPeak_w190814_r00.npy')\n",
    "\n",
    "mock_q_w190814 = np.zeros((len(posteriors_w190814),n_catalogs))\n",
    "mock_x_w190814 = np.zeros((len(posteriors_w190814),n_catalogs))\n",
    "\n",
    "resampled_q_w190814 = np.zeros((len(posteriors_w190814),n_catalogs))\n",
    "resampled_x_w190814 = np.zeros((len(posteriors_w190814),n_catalogs))\n",
    "\n",
    "mu_chi_evol_w190814 = np.zeros(n_catalogs)\n",
    "logsig_chi_evol_w190814 = np.zeros(n_catalogs)\n",
    "alpha_evol_w190814 = np.zeros(n_catalogs)\n",
    "beta_evol_w190814 = np.zeros(n_catalogs)\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    samp = np.random.choice(np.arange(samps_w190814.shape[0]))\n",
    "    \n",
    "    mMin = 2.5\n",
    "    lmbda = samps_w190814[samp,0]\n",
    "    mMax = samps_w190814[samp,1]\n",
    "    m0 = samps_w190814[samp,2]\n",
    "    sigM = samps_w190814[samp,3]\n",
    "    fPeak = samps_w190814[samp,4]\n",
    "    bq = samps_w190814[samp,5]\n",
    "    kappa = samps_w190814[samp,6]\n",
    "    mu0 = samps_w190814[samp,7]\n",
    "    logsig0 = samps_w190814[samp,8]\n",
    "    alpha = samps_w190814[samp,9]\n",
    "    beta = samps_w190814[samp,10]\n",
    "    \n",
    "    mu_chi_evol_w190814[i] = mu0\n",
    "    logsig_chi_evol_w190814[i] = logsig0    \n",
    "    alpha_evol_w190814[i] = alpha\n",
    "    beta_evol_w190814[i] = beta\n",
    "    \n",
    "    p_det_m1_pl = (1.+lmbda)*m1_det**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "    p_det_m1_pl[m1_det>mMax] = 0\n",
    "    p_det_m1_peak = np.exp(-0.5*(m1_det-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "    mock_p_m1 = fPeak*p_det_m1_peak + (1.-fPeak)*p_det_m1_pl  \n",
    "    \n",
    "    mock_p_m2 = (1.+bq)*np.power(m2_det,bq)/(np.power(m1_det,1.+bq)-mMin**(1.+bq))\n",
    "    mock_p_m2[m2_det<mMin]=0\n",
    "    \n",
    "    mock_p_z = np.power(1.+z_det,kappa-1.)\n",
    "    \n",
    "    mu = mu0 + alpha*(q_det-0.5)\n",
    "    logsig = logsig0 + beta*(q_det-0.5)\n",
    "    mock_p_chi = calculate_Gaussian(X_det, mu, 10.**(2.*logsig),-1.,1.)\n",
    "    \n",
    "    p_det = mock_p_m1*mock_p_m2*mock_p_z*mock_p_chi*pop_reweight\n",
    "    p_det /= np.sum(p_det)\n",
    "    detected_injections = np.random.choice(np.arange(m1_det.size),size=len(posteriors_w190814),p=p_det,replace=False)\n",
    "    mock_q_w190814[:,i] = q_det[detected_injections]\n",
    "    mock_x_w190814[:,i] = X_det[detected_injections]\n",
    "\n",
    "    for ii,key in enumerate(posteriors_w190814):\n",
    "\n",
    "        chis = posteriors_w190814[key]['Xeff']\n",
    "        Xeff_prior = posteriors_w190814[key]['Xeff_priors']\n",
    "        m1s = posteriors_w190814[key]['m1']\n",
    "        m2s = posteriors_w190814[key]['m2']\n",
    "        zs = posteriors_w190814[key]['z']\n",
    "        weights = posteriors_w190814[key]['weights']\n",
    "        qs = m2s/m1s\n",
    "        \n",
    "        mu = mu0 + alpha*(qs-0.5)\n",
    "        logsig = logsig0 + beta*(qs-0.5)\n",
    "        p_Chi = calculate_Gaussian(chis, mu, 10.**(2.*logsig),-1.,1.)\n",
    "\n",
    "        # p(m1)\n",
    "        p_m1_pl = (1.+lmbda)*m1s**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "        p_m1_pl[m1s>mMax] = 0.\n",
    "        p_m1_peak = np.exp(-0.5*(m1s-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "        p_m1 = fPeak*p_m1_peak + (1.-fPeak)*p_m1_pl\n",
    "        old_m1_prior = np.ones(m1s.size)\n",
    "\n",
    "        # p(m2)\n",
    "        p_m2 = (1.+bq)*np.power(m2s,bq)/(np.power(m1s,1.+bq)-mMin**(1.+bq))\n",
    "        old_m2_prior = np.ones(m1s.size)\n",
    "        p_m2[m2s<mMin]=0\n",
    "\n",
    "        # p(z)\n",
    "        p_z = np.power(1.+zs,kappa-1.)\n",
    "        old_pz_prior = (1.+zs)**(2.7-1.)\n",
    "        \n",
    "        # Draw probs\n",
    "        probs = p_Chi*p_m1*p_m2*p_z*weights/Xeff_prior/old_m1_prior/old_m2_prior/old_pz_prior\n",
    "        \n",
    "        if np.any(probs!=probs):\n",
    "            print(np.where(probs!=probs))\n",
    "            print(probs)\n",
    "        probs[probs<0] = 0.\n",
    "        probs /= np.sum(probs)\n",
    "        chosenInd = np.random.choice(np.arange(m1s.size),p=probs)\n",
    "        \n",
    "        resampled_q_w190814[ii,i] = qs[chosenInd]\n",
    "        resampled_x_w190814[ii,i] = chis[chosenInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reweighted_sample_dict = {\n",
    "    'mock_q':mock_q_w190814,\n",
    "    'mock_chi':mock_x_w190814,\n",
    "    'resampled_q':resampled_q_w190814,\n",
    "    'resampled_chi':resampled_x_w190814\n",
    "}\n",
    "\n",
    "np.save('./reweighted_samples_yesEvolution_w190814.npy',reweighted_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 5. Correlated population with Gaussian p(q|m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_catalogs = 750\n",
    "posteriors = np.load(\"../input/sampleDict.pickle\",allow_pickle=True)\n",
    "samps_gaussianQ = np.load('./../code/output/processed_emcee_samples_plPeak_gaussianQ_r00.npy')\n",
    "\n",
    "mock_q_gaussianQ = np.zeros((len(posteriors),n_catalogs))\n",
    "mock_x_gaussianQ = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_q_gaussianQ = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_x_gaussianQ = np.zeros((len(posteriors),n_catalogs))\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    samp = np.random.choice(np.arange(samps_gaussianQ.shape[0]))\n",
    "    \n",
    "    mMin = 5.\n",
    "    lmbda = samps_gaussianQ[samp,0]\n",
    "    mMax = samps_gaussianQ[samp,1]\n",
    "    m0 = samps_gaussianQ[samp,2]\n",
    "    sigM = samps_gaussianQ[samp,3]\n",
    "    fPeak = samps_gaussianQ[samp,4]\n",
    "    mu_q = samps_gaussianQ[samp,5]\n",
    "    sig_q = samps_gaussianQ[samp,6]\n",
    "    kappa = samps_gaussianQ[samp,7]\n",
    "    mu0 = samps_gaussianQ[samp,8]\n",
    "    logsig0 = samps_gaussianQ[samp,9]\n",
    "    alpha = samps_gaussianQ[samp,10]\n",
    "    beta = samps_gaussianQ[samp,11]\n",
    "    \n",
    "    p_det_m1_pl = (1.+lmbda)*m1_det**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "    p_det_m1_pl[m1_det>mMax] = 0\n",
    "    p_det_m1_peak = np.exp(-0.5*(m1_det-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "    mock_p_m1 = fPeak*p_det_m1_peak + (1.-fPeak)*p_det_m1_pl  \n",
    "    \n",
    "    mock_p_m2 = calculate_Gaussian(q_det,mu_q,sig_q,mMin/m1_det,1.)/m1_det\n",
    "    mock_p_m2[m2_det<mMin]=0\n",
    "    \n",
    "    mock_p_z = np.power(1.+z_det,kappa-1.)\n",
    "    \n",
    "    mu = mu0 + alpha*(q_det-0.5)\n",
    "    logsig = logsig0 + beta*(q_det-0.5)\n",
    "    mock_p_chi = calculate_Gaussian(X_det, mu, 10.**(2.*logsig),-1.,1.)\n",
    "    \n",
    "    p_det = mock_p_m1*mock_p_m2*mock_p_z*mock_p_chi*pop_reweight\n",
    "    p_det /= np.sum(p_det)\n",
    "    detected_injections = np.random.choice(np.arange(m1_det.size),size=len(posteriors),p=p_det,replace=False)\n",
    "    mock_q_gaussianQ[:,i] = q_det[detected_injections]\n",
    "    mock_x_gaussianQ[:,i] = X_det[detected_injections]\n",
    "\n",
    "    for ii,key in enumerate(posteriors):\n",
    "\n",
    "        chis = posteriors[key]['Xeff']\n",
    "        Xeff_prior = posteriors[key]['Xeff_priors']\n",
    "        m1s = posteriors[key]['m1']\n",
    "        m2s = posteriors[key]['m2']\n",
    "        zs = posteriors[key]['z']\n",
    "        weights = posteriors[key]['weights']\n",
    "        qs = m2s/m1s\n",
    "        \n",
    "        mu = mu0 + alpha*(qs-0.5)\n",
    "        logsig = logsig0 + beta*(qs-0.5)\n",
    "        p_Chi = calculate_Gaussian(chis, mu, 10.**(2.*logsig),-1.,1.)\n",
    "\n",
    "        # p(m1)\n",
    "        p_m1_pl = (1.+lmbda)*m1s**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "        p_m1_pl[m1s>mMax] = 0.\n",
    "        p_m1_peak = np.exp(-0.5*(m1s-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "        p_m1 = fPeak*p_m1_peak + (1.-fPeak)*p_m1_pl\n",
    "        old_m1_prior = np.ones(m1s.size)\n",
    "\n",
    "        # p(m2)\n",
    "        p_m2 = calculate_Gaussian(qs,mu_q,sig_q,mMin/m1s,1.)/m1s\n",
    "        old_m2_prior = np.ones(m1s.size)\n",
    "        p_m2[m2s<mMin]=0\n",
    "\n",
    "        # p(z)\n",
    "        p_z = np.power(1.+zs,kappa-1.)\n",
    "        old_pz_prior = (1.+zs)**(2.7-1.)\n",
    "        \n",
    "        # Draw probs\n",
    "        probs = p_Chi*p_m1*p_m2*p_z*weights/Xeff_prior/old_m1_prior/old_m2_prior/old_pz_prior\n",
    "        \n",
    "        if np.any(probs!=probs):\n",
    "            print(np.where(probs!=probs))\n",
    "            print(probs)\n",
    "        probs[probs<0] = 0.\n",
    "        probs /= np.sum(probs)\n",
    "        chosenInd = np.random.choice(np.arange(m1s.size),p=probs)\n",
    "        \n",
    "        resampled_q_gaussianQ[ii,i] = qs[chosenInd]\n",
    "        resampled_x_gaussianQ[ii,i] = chis[chosenInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reweighted_sample_dict = {\n",
    "    'mock_q':mock_q_gaussianQ,\n",
    "    'mock_chi':mock_x_gaussianQ,\n",
    "    'resampled_q':resampled_q_gaussianQ,\n",
    "    'resampled_chi':resampled_x_gaussianQ\n",
    "}\n",
    "\n",
    "np.save('./reweighted_samples_yesEvolution_gaussianQ.npy',reweighted_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 6. Mixture of two bivariate Gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_catalogs = 750\n",
    "posteriors = np.load(\"../input/sampleDict.pickle\",allow_pickle=True)\n",
    "samps_bivariateGaussian = np.load('./../code/output/processed_emcee_samples_plPeak_bivariateGaussian_r05.npy')\n",
    "\n",
    "mock_q_bivariateGaussian = np.zeros((len(posteriors),n_catalogs))\n",
    "mock_x_bivariateGaussian = np.zeros((len(posteriors),n_catalogs))\n",
    "resampled_q_bivariateGaussian = np.zeros((len(posteriors),400))\n",
    "resampled_x_bivariateGaussian = np.zeros((len(posteriors),400))\n",
    "\n",
    "for i in range(n_catalogs):\n",
    "    \n",
    "    samp = np.random.choice(np.arange(samps_bivariateGaussian.shape[0]))\n",
    "    \n",
    "    mMin = 5.\n",
    "    lmbda = samps_bivariateGaussian[samp,0]\n",
    "    mMax = samps_bivariateGaussian[samp,1]\n",
    "    m0 = samps_bivariateGaussian[samp,2]\n",
    "    sigM = samps_bivariateGaussian[samp,3]\n",
    "    fPeak = samps_bivariateGaussian[samp,4]\n",
    "    f_lowSpin = samps_bivariateGaussian[samp,5]\n",
    "    mu_q_lowSpin = samps_bivariateGaussian[samp,6]\n",
    "    logsig_q_lowSpin = samps_bivariateGaussian[samp,7]\n",
    "    mu_q_highSpin = samps_bivariateGaussian[samp,8]\n",
    "    logsig_q_highSpin = samps_bivariateGaussian[samp,9]\n",
    "    kappa = samps_bivariateGaussian[samp,10]\n",
    "    mu_chi_lowSpin = samps_bivariateGaussian[samp,11]\n",
    "    logsig_chi_lowSpin = samps_bivariateGaussian[samp,12]\n",
    "    mu_chi_highSpin = samps_bivariateGaussian[samp,13]\n",
    "    logsig_chi_highSpin = samps_bivariateGaussian[samp,14]\n",
    "\n",
    "    # p(m1)\n",
    "    p_det_m1_pl = (1.+lmbda)*m1_det**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "    p_det_m1_pl[m1_det>mMax] = 0.\n",
    "    p_det_m1_peak = np.exp(-0.5*(m1_det-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "    mock_p_m1 = fPeak*p_det_m1_peak + (1.-fPeak)*p_det_m1_pl\n",
    "\n",
    "    # p(m2)\n",
    "    mock_p_Xeff_m2_lowSpin = calculate_Gaussian_2D(X_det,q_det,mu_chi_lowSpin,10.**(2.*logsig_chi_lowSpin),\\\n",
    "                mu_q_lowSpin,10.**(2.*logsig_q_lowSpin),-1.,1.,0.,1.)/m1_det\n",
    "    mock_p_Xeff_m2_highSpin = calculate_Gaussian_2D(X_det,q_det,mu_chi_highSpin,10.**(2.*logsig_chi_highSpin),\\\n",
    "                mu_q_highSpin,10.**(2.*logsig_q_highSpin),-1.,1.,0.,1.)/m1_det\n",
    "    mock_p_Xeff_m2 = f_lowSpin*mock_p_Xeff_m2_lowSpin + (1.-f_lowSpin)*mock_p_Xeff_m2_highSpin\n",
    "    mock_p_Xeff_m2[m2_det<mMin] = 0.\n",
    "\n",
    "    # p(z)\n",
    "    p_z = np.power(1.+z_det,kappa-1.)\n",
    "    \n",
    "    p_det = mock_p_m1*mock_p_Xeff_m2*mock_p_z*pop_reweight\n",
    "    p_det /= np.sum(p_det)\n",
    "    detected_injections = np.random.choice(np.arange(m1_det.size),size=len(posteriors),p=p_det,replace=False)\n",
    "    mock_q_bivariateGaussian[:,i] = q_det[detected_injections]\n",
    "    mock_x_bivariateGaussian[:,i] = X_det[detected_injections]\n",
    "\n",
    "    for ii,key in enumerate(posteriors):\n",
    "\n",
    "        chis = posteriors[key]['Xeff']\n",
    "        Xeff_prior = posteriors[key]['Xeff_priors']\n",
    "        m1s = posteriors[key]['m1']\n",
    "        m2s = posteriors[key]['m2']\n",
    "        zs = posteriors[key]['z']\n",
    "        weights = posteriors[key]['weights']\n",
    "        qs = m2s/m1s\n",
    "\n",
    "        # p(m1)\n",
    "        p_m1_pl = (1.+lmbda)*m1s**lmbda/(mMax**(1.+lmbda) - mMin**(1.+lmbda))\n",
    "        p_m1_pl[m1s>mMax] = 0.\n",
    "        p_m1_peak = np.exp(-0.5*(m1s-m0)**2./sigM**2)/np.sqrt(2.*np.pi*sigM**2.)\n",
    "        p_m1 = fPeak*p_m1_peak + (1.-fPeak)*p_m1_pl\n",
    "        old_m1_prior = np.ones(m1s.size)\n",
    "        old_m1_prior = np.ones(m2s.size)\n",
    "\n",
    "        # p(m2)\n",
    "        p_Xeff_m2_lowSpin = calculate_Gaussian_2D(chis,qs,mu_chi_lowSpin,10.**(2.*logsig_chi_lowSpin),\\\n",
    "                    mu_q_lowSpin,10.**(2.*logsig_q_lowSpin),-1.,1.,0.,1.)/m1s\n",
    "        p_Xeff_m2_highSpin = calculate_Gaussian_2D(chis,qs,mu_chi_highSpin,10.**(2.*logsig_chi_highSpin),\\\n",
    "                    mu_q_highSpin,10.**(2.*logsig_q_highSpin),-1.,1.,0.,1.)/m1s\n",
    "        p_Xeff_m2 = f_lowSpin*p_Xeff_m2_lowSpin + (1.-f_lowSpin)*p_Xeff_m2_highSpin\n",
    "        p_Xeff_m2[m2s<mMin] = 0.\n",
    "\n",
    "        # p(z)\n",
    "        p_z = np.power(1.+zs,kappa-1.)\n",
    "        old_pz_prior = (1.+zs)**(2.7-1.)\n",
    "        \n",
    "        # Draw probs\n",
    "        probs = p_Xeff_m2*p_m1*p_z*weights/Xeff_prior/old_m1_prior/old_m2_prior/old_pz_prior\n",
    "        \n",
    "        if np.any(probs!=probs):\n",
    "            print(np.where(probs!=probs))\n",
    "            print(probs)\n",
    "        probs[probs<0] = 0.\n",
    "        probs /= np.sum(probs)\n",
    "        chosenInd = np.random.choice(np.arange(m1s.size),p=probs)\n",
    "        resampled_q_bivariateGaussian[ii,i] = qs[chosenInd]\n",
    "        resampled_x_bivariateGaussian[ii,i] = chis[chosenInd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reweighted_sample_dict = {\n",
    "    'mock_q':mock_q_bivariateGaussian,\n",
    "    'mock_chi':mock_x_bivariateGaussian,\n",
    "    'resampled_q':resampled_q_bivariateGaussian,\n",
    "    'resampled_chi':resampled_x_bivariateGaussian\n",
    "}\n",
    "\n",
    "np.save('./reweighted_samples_yesEvolution_bivariateGaussian.npy',reweighted_sample_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 7. Injection study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load population samples from injected population\n",
    "injection_samps_plPeak = np.load('../injection-study/processed_emcee_samples_injection_plPeak_r02.npy')\n",
    "n_catalogs = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../injection-study/output/job_02121_result.json\n",
      "1728.9184110943913\n",
      "../injection-study/output/job_02405_result.json\n",
      "3653.1037427590154\n",
      "../injection-study/output/job_02688_result.json\n",
      "1481.7538133372257\n",
      "../injection-study/output/job_03611_result.json\n",
      "3110.364279252653\n",
      "../injection-study/output/job_04003_result.json\n",
      "1727.5004377801029\n",
      "../injection-study/output/job_04158_result.json\n",
      "1856.9409223072234\n",
      "../injection-study/output/job_04546_result.json\n",
      "2830.891522314026\n",
      "../injection-study/output/job_06677_result.json\n",
      "2655.8450435268924\n",
      "../injection-study/output/job_07289_result.json\n",
      "1401.2150662585702\n",
      "../injection-study/output/job_07842_result.json\n",
      "1671.051986194712\n",
      "../injection-study/output/job_08792_result.json\n",
      "1951.7784121140905\n",
      "../injection-study/output/job_08909_result.json\n",
      "838.7770190479383\n",
      "../injection-study/output/job_08918_result.json\n",
      "2430.6463367490737\n",
      "../injection-study/output/job_09738_result.json\n",
      "1574.1616526296534\n",
      "../injection-study/output/job_11231_result.json\n",
      "1834.153418709552\n",
      "../injection-study/output/job_11417_result.json\n",
      "1402.9716332694395\n",
      "../injection-study/output/job_12838_result.json\n",
      "1523.308132768508\n",
      "../injection-study/output/job_14121_result.json\n",
      "5227.018080660696\n",
      "../injection-study/output/job_15322_result.json\n",
      "1593.647355704731\n",
      "../injection-study/output/job_15356_result.json\n",
      "1873.787399920594\n",
      "../injection-study/output/job_16007_result.json\n",
      "2549.4401290141295\n",
      "../injection-study/output/job_16280_result.json\n",
      "1451.7190747186385\n",
      "../injection-study/output/job_16297_result.json\n",
      "2539.0639532031696\n",
      "../injection-study/output/job_19853_result.json\n",
      "1022.6065119306156\n",
      "../injection-study/output/job_20238_result.json\n",
      "2190.8958739864597\n",
      "../injection-study/output/job_20456_result.json\n",
      "5997.000593684132\n",
      "../injection-study/output/job_21979_result.json\n",
      "1997.0424485889514\n",
      "../injection-study/output/job_23031_result.json\n",
      "1478.724999314492\n",
      "../injection-study/output/job_24352_result.json\n",
      "1916.5499979538708\n",
      "../injection-study/output/job_27214_result.json\n",
      "2183.9546695403287\n",
      "../injection-study/output/job_27236_result.json\n",
      "2306.595669697915\n",
      "../injection-study/output/job_27913_result.json\n",
      "728.2924172408242\n",
      "../injection-study/output/job_27984_result.json\n",
      "1415.4151290145248\n",
      "../injection-study/output/job_28018_result.json\n",
      "2627.889241936148\n",
      "../injection-study/output/job_28327_result.json\n",
      "3035.6537127992565\n",
      "../injection-study/output/job_29266_result.json\n",
      "1890.0020096200656\n",
      "../injection-study/output/job_29624_result.json\n",
      "1427.3835015357074\n",
      "../injection-study/output/job_32545_result.json\n",
      "2909.0377245399527\n",
      "../injection-study/output/job_32925_result.json\n",
      "1358.8302567232363\n",
      "../injection-study/output/job_36773_result.json\n",
      "1956.3189317606368\n",
      "../injection-study/output/job_43454_result.json\n",
      "1382.0694140891137\n",
      "../injection-study/output/job_44849_result.json\n",
      "1872.3974864790716\n",
      "../injection-study/output/job_46543_result.json\n",
      "2708.288972486338\n",
      "../injection-study/output/job_46717_result.json\n",
      "1313.8170408261685\n",
      "../injection-study/output/job_47744_result.json\n",
      "1738.7483052475204\n",
      "../injection-study/output/job_48062_result.json\n",
      "2111.773202764929\n",
      "../injection-study/output/job_48467_result.json\n",
      "1783.35581536306\n",
      "../injection-study/output/job_49062_result.json\n",
      "1614.177532838562\n",
      "../injection-study/output/job_49117_result.json\n",
      "1841.549183109668\n",
      "../injection-study/output/job_49312_result.json\n",
      "2005.651904936785\n"
     ]
    }
   ],
   "source": [
    "# Define a function to reweight to an isotropic prior on s1z and s2z, rather than an aligned prior\n",
    "# This is done for consistency with the GWTC-2 PE samples\n",
    "def isotropic_pz(szs,aMax):\n",
    "    return np.log(aMax/np.abs(szs))/(2.*aMax)\n",
    "\n",
    "reweightedSampleDictInjected = {}\n",
    "bilby_output_files = np.sort(glob.glob('../injection-study/output/job_?????_result.json'))\n",
    "for i,f in enumerate(bilby_output_files):\n",
    "    \n",
    "    with open(f,'r') as jf:\n",
    "        result = json.load(jf)\n",
    "\n",
    "    m1 = np.array(result['posterior']['content']['mass_1_source'])\n",
    "    m2 = np.array(result['posterior']['content']['mass_2_source'])\n",
    "    s1 = np.array(result['posterior']['content']['spin_1z'])\n",
    "    s2 = np.array(result['posterior']['content']['spin_2z'])\n",
    "    \n",
    "    qs = m2/m1\n",
    "    chis = (m1*s1 + m2*s2)/(m1+m2)\n",
    "    \n",
    "    p_szs = isotropic_pz(s1,1.)*isotropic_pz(s2,1.)\n",
    "    draw_probs = p_szs/np.sum(p_szs)\n",
    "    \n",
    "    chosenInds = np.random.choice(np.arange(qs.size),size=2000,replace=True,p=draw_probs)\n",
    "    qs = qs[chosenInds]\n",
    "    chis = chis[chosenInds]\n",
    "    \n",
    "    reweightedSampleDictInjected[i] = {'q':qs,'x':chis}\n",
    "    \n",
    "    print(f)\n",
    "    print(1./np.max(draw_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.save(\"injection_samples_reweightedToIsotropy.npy\",reweightedSampleDictInjected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "igwn-py37",
   "language": "python",
   "name": "igwn-py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
